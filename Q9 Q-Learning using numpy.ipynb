{"cells":[{"cell_type":"markdown","metadata":{"id":"FqNwB6FSGeku"},"source":["## Accompanying notebook for the FloydHub article: \"An introduction to Q-Learning: Reinforcement Learning\""]},{"cell_type":"markdown","metadata":{"id":"KmiAeW1EGekx"},"source":["![](https://i.ibb.co/c8LXj7X/Capture.png)\n","\n","<center>The environment</center>"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"bKSctmEGGeky","executionInfo":{"status":"ok","timestamp":1647987280429,"user_tz":240,"elapsed":226,"user":{"displayName":"Melissa Hunfalvay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13329392479802854613"}}},"outputs":[],"source":["# Only numpy\n","import numpy as np"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"W0F2g8e_Gekz","executionInfo":{"status":"ok","timestamp":1647987282129,"user_tz":240,"elapsed":213,"user":{"displayName":"Melissa Hunfalvay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13329392479802854613"}}},"outputs":[],"source":["# Initialize parameters\n","gamma = 0.9 # Discount factor \n","alpha = 0.75 # Learning rate "]},{"cell_type":"code","execution_count":3,"metadata":{"id":"lEWRayzcGekz","executionInfo":{"status":"ok","timestamp":1647987284340,"user_tz":240,"elapsed":222,"user":{"displayName":"Melissa Hunfalvay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13329392479802854613"}}},"outputs":[],"source":["# Define the states\n","location_to_state = {\n","    'L1' : 0,\n","    'L2' : 1,\n","    'L3' : 2,\n","    'L4' : 3,\n","    'L5' : 4,\n","    'L6' : 5,\n","    'L7' : 6,\n","    'L8' : 7,\n","    'L9' : 8,\n","    'L10': 9\n","}"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ZJkqkIEVGek0","executionInfo":{"status":"ok","timestamp":1647987286644,"user_tz":240,"elapsed":226,"user":{"displayName":"Melissa Hunfalvay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13329392479802854613"}}},"outputs":[],"source":["# Define the actions\n","actions = [0,1,2,3,4,5,6,7,8,9]"]},{"cell_type":"markdown","metadata":{"id":"aRULV3k9Gek0"},"source":["![](https://i.ibb.co/k4kgnQS/Capture.png)\n","\n","<center>Rewards' table</center>"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"WGUjbhGvGek1","executionInfo":{"status":"ok","timestamp":1647987374137,"user_tz":240,"elapsed":2,"user":{"displayName":"Melissa Hunfalvay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13329392479802854613"}}},"outputs":[],"source":["# Define the rewards\n","rewards = np.array([[0,1,0,0,0,0,0,0,0,0],\n","              [1,0,1,0,0,0,0,0,0,0],\n","              [0,1,0,0,0,1,0,0,0,0],\n","              [0,0,0,0,0,0,1,0,0,0],\n","              [0,1,0,0,0,0,0,1,0,0],\n","              [0,0,1,0,0,0,0,0,0,0],\n","              [0,0,0,1,0,0,0,1,0,0],\n","              [0,0,0,0,1,0,1,0,1,0],\n","              [0,0,0,0,0,0,0,1,0,1],\n","              [0,0,0,0,0,0,0,0,1,0]])"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"4xfexOAlGek2","executionInfo":{"status":"ok","timestamp":1647987379051,"user_tz":240,"elapsed":552,"user":{"displayName":"Melissa Hunfalvay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13329392479802854613"}}},"outputs":[],"source":["# Maps indices to locations\n","state_to_location = dict((state,location) for location,state in location_to_state.items())"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"K6b6JdBgGek2","executionInfo":{"status":"ok","timestamp":1647987381700,"user_tz":240,"elapsed":214,"user":{"displayName":"Melissa Hunfalvay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13329392479802854613"}}},"outputs":[],"source":["# Define the actions\n","actions = [0,1,2,3,4,5,6,7,8,9]"]},{"cell_type":"markdown","metadata":{"id":"OahXrYn9Gek3"},"source":["The following function is going to take two arguments: \n","\n","- starting location in the warehouse and \n","- end location in the warehouse respectively \n","\n","It will return the optimal route for reaching the end location from the starting location in the form of an ordered list (containing the letters)."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"6YuQ3nYjGek3","executionInfo":{"status":"ok","timestamp":1647987638423,"user_tz":240,"elapsed":231,"user":{"displayName":"Melissa Hunfalvay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13329392479802854613"}}},"outputs":[],"source":["def get_optimal_route(start_location,end_location):\n","    # Copy the rewards matrix to new Matrix\n","    rewards_new = np.copy(rewards)\n","    # Get the ending state corresponding to the ending location as given\n","    ending_state = location_to_state[end_location]\n","    # With the above information automatically set the priority of the given ending state to the highest one\n","    rewards_new[ending_state,ending_state] = 999\n","\n","    # -----------Q-Learning algorithm-----------\n","   \n","    # Initializing Q-Values\n","    Q = np.array(np.zeros([10,10]))\n","\n","    # Q-Learning process\n","    for i in range(1000):\n","        # Pick up a state randomly\n","        current_state = np.random.randint(0,10) # Python excludes the upper bound\n","        # For traversing through the neighbor locations in the maze\n","        playable_actions = []\n","        # Iterate through the new rewards matrix and get the actions > 0\n","        for j in range(10):\n","            if rewards_new[current_state,j] > 0:\n","                playable_actions.append(j)\n","        # Pick an action randomly from the list of playable actions leading us to the next state\n","        next_state = np.random.choice(playable_actions)\n","        # Compute the temporal difference\n","        # The action here exactly refers to going to the next state\n","        TD = rewards_new[current_state,next_state] + gamma * Q[next_state, np.argmax(Q[next_state,])] - Q[current_state,next_state]\n","        # Update the Q-Value using the Bellman equation\n","        Q[current_state,next_state] += alpha * TD\n","\n","    # Initialize the optimal route with the starting location\n","    route = [start_location]\n","    # We do not know about the next location yet, so initialize with the value of starting location\n","    next_location = start_location\n","    \n","    # We don't know about the exact number of iterations needed to reach to the final location hence while loop will be a good choice for iteratiing\n","    while(next_location != end_location):\n","        # Fetch the starting state\n","        starting_state = location_to_state[start_location]\n","        # Fetch the highest Q-value pertaining to starting state\n","        next_state = np.argmax(Q[starting_state,])\n","        # We got the index of the next state. But we need the corresponding letter. \n","        next_location = state_to_location[next_state]\n","        route.append(next_location)\n","        # Update the starting location for the next iteration\n","        start_location = next_location\n","    \n","    return route"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"jPySO4rAGek4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647987640809,"user_tz":240,"elapsed":228,"user":{"displayName":"Melissa Hunfalvay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13329392479802854613"}},"outputId":"ff5d798c-4cdc-43b0-f08b-011b18558a41"},"outputs":[{"output_type":"stream","name":"stdout","text":["['L10', 'L9', 'L8', 'L7', 'L4']\n"]}],"source":["print(get_optimal_route('L10', 'L4'))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"name":"Q9 Q-Learning using numpy.ipynb","provenance":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}